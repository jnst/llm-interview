id: "Q37"
question: "混合エキスパート（MoE）はLLMのスケーラビリティをどのように向上させるか？"
answer: "MoEは、ゲーティング関数を使用して入力ごとに特定のエキスパートサブネットワークを活性化し、計算負荷を削減します。例えば、モデルのパラメータの10%のみがクエリごとに使用される可能性があり、高いパフォーマンスを維持しながら10億パラメータモデルの効率的な動作を可能にします。"