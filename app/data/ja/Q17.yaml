id: "Q17"
question: "トランスフォーマーは従来のSeq2Seqモデルをどのように改善するか？"
answer: "トランスフォーマーは、セルフアテンションが同時トークン処理を可能にする並列処理、アテンションが離れたトークンの関係を捉える長距離依存性、シーケンスの順序を保持する位置エンコーディングによってSeq2Seqの制限を克服し、これらの機能により、翻訳などのタスクにおけるスケーラビリティとパフォーマンスが向上します。"