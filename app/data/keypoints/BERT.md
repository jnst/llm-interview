---
title: BERT
contexts:
  - ai
---

## BERT

<Context name="ai">

### 概要と基本原理
BERTは「Bidirectional Encoder Representations from Transformers」の略で、文脈を双方向から理解する画期的な言語モデルです。従来の言語モデルが左から右へと一方向に文字を読むのに対し、BERTは文の前後両方向から同時に文脈を理解します。これは本を読むときに前後の文脈を参照して理解するという人間の読み方に近いアプローチです。

### 技術的特徴
BERTの核心は「マスク言語モデリング（MLM）」という学習手法にあります。これは文中の一部の単語を意図的に隠し（マスク）、前後の文脈からその単語を予測するタスクです。さらに「次文予測（NSP）」タスクで文章間の関係も学習します。この双方向学習により、BERTは文脈依存の豊富な言語表現を獲得できます。

### 応用可能性
BERTは事前学習された表現を様々なタスクに転用できる「転移学習」の優れた例です。質問応答、感情分析、文書分類、固有表現抽出など、多様な自然言語処理タスクで最小限のタスク固有の調整（ファインチューニング）で高い性能を発揮します。

### 他の技術との関連
BERTはTransformerアーキテクチャのエンコーダ部分のみを使用し、GPTシリーズ（デコーダのみ）との対比で語られることが多いです。また、BERTの成功により、RoBERTa、ALBERT、DeBERTaなど多くの改良版が開発され、現在の大規模言語モデルの基盤技術となっています。

### なぜ重要なのか
BERTが重要な理由は、自然言語理解における「文脈の重要性」を実証したことです。同じ単語でも文脈によって意味が変わるという言語の本質的特徴を、機械学習モデルで初めて効果的に捉えることができました。これにより、AIの言語理解能力が飛躍的に向上し、現在の言語AI技術の基盤を築きました。

</Context>
