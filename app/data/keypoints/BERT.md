---
title: BERT
contexts:
  - ai
---

<Context name="ai">

## BERT

BERTは「Bidirectional Encoder Representations from Transformers」の略で、文章の文脈を双方向から理解する事前学習済み言語モデルです。

### なぜ双方向性が重要なのか

従来の言語モデルは左から右へ、または右から左へという一方向でしか文脈を理解できませんでした。しかし「彼は銀行に行った」という文の「銀行」が金融機関なのか川岸なのかを判断するには、前後両方の文脈が必要です。BERTはマスク言語モデル（MLM）という手法により、この双方向理解を実現しました。

### コア原理：マスク言語モデル（MLM）

1. **ランダムマスキング**: 入力文の15%の単語を[MASK]トークンに置換
2. **予測タスク**: マスクされた単語を周囲の文脈から予測
3. **双方向学習**: 前後両方の文脈を同時に考慮して学習

### 他の知識との関連

- **Transformerアーキテクチャ**: エンコーダー部分のみを使用（デコーダーは不使用）
- **Self-Attention**: 文中の全単語間の関係性を捉える
- **転移学習**: 事前学習→ファインチューニングのパラダイム確立
- **GPTとの対比**: GPTは自己回帰的（左→右）、BERTは双方向

### 長期記憶のための概念理解

BERTを「文章の穴埋め問題の達人」と理解すると覚えやすい。人間が文章を読む時のように、前後の文脈から意味を理解する能力を機械学習で実現したモデルです。

### 応用可能性

1. **文書分類**: スパム判定、感情分析
2. **質問応答**: 文章から答えを抽出
3. **固有表現認識**: 人名・地名の識別
4. **文の類似度判定**: 意味的に似た文の発見

この双方向理解の原理は、後のRoBERTa、ALBERT、ELECTRAなど多くの派生モデルの基礎となり、現在のLLMの発展にも大きく寄与しています。

</Context>
