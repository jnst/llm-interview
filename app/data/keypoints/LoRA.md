---
title: LoRA
contexts:
  - ai
---

<Context name="ai">

## LoRA

LoRA（Low-Rank Adaptation）は、大規模言語モデルを効率的にファインチューニングするための革新的手法で、パラメータ数を劇的に削減しながら高い性能を実現します。

### なぜLoRAが画期的なのか

従来、大規模モデルのファインチューニングには全パラメータの更新が必要で、膨大な計算資源とメモリが必要でした。LoRAは「重み行列の変化は低ランクである」という洞察に基づき、わずか0.1%程度のパラメータで同等の性能を達成します。

### コア原理：低ランク分解

元の重み行列Wの更新を低ランク行列の積で表現：
```
W' = W + ΔW = W + BA
```

ここで：
- **B**: d×r の行列（ダウンプロジェクション）
- **A**: r×k の行列（アッププロジェクション）
- **r**: ランク（通常4〜64、元の次元数より遥かに小さい）

### 技術的な利点

1. **メモリ効率**: 学習パラメータが1000分の1以下に
2. **計算効率**: 勾配計算が低次元空間で実行
3. **モジュール性**: 複数のLoRAアダプタを切り替え可能
4. **推論時の統合**: BA積を事前計算して元の重みに加算

### 他の知識との関連

- **行列の低ランク近似**: SVD（特異値分解）の応用
- **アダプター層**: より軽量な転移学習手法
- **プロンプトチューニング**: パラメータ効率的手法の別アプローチ
- **QLoRA**: 量子化と組み合わせた更なる効率化

### 長期記憶のための概念理解

LoRAを「専門知識の差分学習」として理解しましょう。基礎モデルという「一般教養」に、特定タスクの「専門知識」を小さな追加モジュールとして学習する仕組みです。

### 実装の要点

```python
# 概念的な実装
class LoRALayer:
    def __init__(self, d, k, r):
        self.A = init_matrix(r, k)  # ランクr
        self.B = init_matrix(d, r)  # ランクr
        
    def forward(self, Wx):
        return Wx + self.B @ self.A @ x  # 低ランク項を追加
```

### 応用シナリオ

**1. マルチタスク対応**：
- 基礎モデル1つ + タスク別LoRAアダプタ複数
- 動的な切り替えによる柔軟な対応

**2. パーソナライゼーション**：
- ユーザー別のLoRAアダプタ
- プライバシーを保護した個別最適化

**3. リソース制約環境**：
- エッジデバイスでのファインチューニング
- 限られたGPUメモリでの学習

### 実践的な知見

**ランク選択の指針**：
- 小さいランク（4-8）: 単純なスタイル適応
- 中程度（16-32）: ドメイン適応
- 大きいランク（64-128）: 複雑なタスク学習

**適用箇所**：
- 主にAttention層のQ、V行列
- 必要に応じてFFN層にも適用

LoRAの成功は「大規模モデルの民主化」を実現し、限られたリソースでも最先端AIを活用できる道を開きました。

</Context>

