---
title: ソフトマックス
contexts:
  - math
---

## ソフトマックス

<Context name="math">

### 概要と基本原理
ソフトマックス関数は、実数値のベクトルを確率分布に変換する数学的関数です。入力値の大きさに応じて0から1の間の値を出力し、全ての出力値の合計が1になるよう正規化します。これは「勝者総取り」ではなく、各選択肢に滑らかな確率を割り当てる「ソフトな最大値選択」を行うため、この名前が付けられました。

### 技術的特徴
ソフトマックス関数は、softmax(x_i) = exp(x_i) / Σ exp(x_j) で定義されます。指数関数を使用することで、入力値の小さな差異を確率の大きな差異に変換します。温度パラメータTを導入すると、softmax(x_i/T)となり、Tが小さいほど鋭い分布、大きいほど滑らかな分布を生成します。数値的安定性のため、通常は最大値を引いてから計算します。

### 応用可能性
ソフトマックス関数は、多クラス分類、言語モデルの次単語予測、アテンション機構の重み計算、強化学習の行動選択など、様々な場面で使用されます。特に、ニューラルネットワークの最終層で確率分布を出力する際の標準的な手法として広く採用されています。また、温度スケーリングにより、モデルの出力の「確信度」を調整することも可能です。

### 他の技術との関連
ソフトマックス関数は、クロスエントロピー損失と組み合わせて使用されることが多く、この組み合わせは勾配計算が簡潔になるという利点があります。また、アテンション機構では、アテンション重みの正規化にソフトマックス関数が使用されます。Gumbel-Softmax、Sparsemax、Entmaxなど、ソフトマックスの変種も開発されています。

### なぜ重要なのか
ソフトマックス関数が重要な理由は、「確率的な選択」を可能にすることです。決定論的な最大値選択ではなく、確率分布を出力することで、モデルの不確実性を表現し、学習プロセスを安定化できます。これにより、分類問題や生成タスクにおいて、より柔軟で信頼性の高いAIシステムの構築が可能になります。

</Context>
