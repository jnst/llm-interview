---
title: クロスエントロピー損失
contexts:
  - optimization
---

## クロスエントロピー損失

<Context name="optimization">

### 概要と基本原理
クロスエントロピー損失は、分類問題における最も重要な損失関数の一つです。真の分布と予測分布の「情報的距離」を測定し、モデルの予測が正解にどの程度近いかを定量化します。これは情報理論の「情報量」の概念に基づいており、予測の不確実性を減らす方向にモデルを学習させる最適化目標となります。

### 技術的特徴
クロスエントロピー損失は、真の分布pと予測分布qに対して、H(p,q) = -Σ p(x) log q(x)で定義されます。二値分類では-[y log(ŷ) + (1-y) log(1-ŷ)]、多クラス分類では-Σ y_i log(ŷ_i)の形をとります。この損失関数は凸関数であり、勾配降下法による最適化が効率的に行えます。また、ソフトマックス関数と組み合わせることで、確率分布を出力する分類器の学習に最適化されています。

### 応用可能性
クロスエントロピー損失は、画像分類、自然言語処理、音声認識など、あらゆる分類タスクで使用されます。深層学習では、ニューラルネットワークの最終層にソフトマックス関数を配置し、クロスエントロピー損失で学習するのが標準的な手法です。また、言語モデルの学習では、次の単語予測タスクにクロスエントロピー損失が用いられ、GPTやBERTなどの大規模言語モデルの基盤となっています。

### 他の技術との関連
クロスエントロピー損失は、KLダイバージェンスと密接な関係があり、実際にはKLダイバージェンスにエントロピー項を加えたものと等価です。また、最大尤度推定の負の対数尤度とも同じ形を持ちます。平均二乗誤差などの他の損失関数と比較して、分類問題では優れた性能を示し、勾配の性質も最適化に適しています。

### なぜ重要なのか
クロスエントロピー損失が重要な理由は、分類問題における「最適な学習目標」を提供することです。情報理論的に意味のある損失関数であり、モデルの予測確率を真の分布に近づける最も効率的な方法を提供します。これにより、現代の深層学習における分類性能の向上と、確率的出力の信頼性向上に不可欠な役割を果たしています。

</Context>
