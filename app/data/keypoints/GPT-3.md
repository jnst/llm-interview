---
title: GPT-3
contexts:
  - ai
---

<Context name="ai">

## GPT-3

GPT-3は1750億個のパラメータを持つ大規模言語モデルで、「規模の法則」と「Few-shot学習」の可能性を実証した画期的なモデルです。

### なぜGPT-3が転換点となったのか

GPT-3以前は、各タスクごとにファインチューニングが必要でした。しかしGPT-3は、タスク説明と数個の例を与えるだけで、追加学習なしに多様なタスクを実行できることを示しました。この「創発的能力」の発見が、現在のLLMブームの起点となっています。

### コア原理：スケーリング則の実証

1. **パラメータ数の飛躍**: GPT-2の15億→GPT-3の1750億（100倍以上）
2. **学習データの規模**: 45TBのテキストデータ（Common Crawl、書籍、Wikipedia等）
3. **計算資源の投入**: 3640ペタFLOPS・日の計算量

### Few-shot学習の革新性

- **Zero-shot**: タスク説明のみで実行
- **One-shot**: 1つの例で学習
- **Few-shot**: 数個の例で高精度達成
- **In-context学習**: プロンプト内で動的に学習

### 他の知識との関連

- **Transformerデコーダー**: 自己回帰的な次単語予測
- **アテンション機構**: 2048トークンの文脈理解
- **スケーリング則**: モデル性能∝パラメータ数^α
- **プロンプトエンジニアリング**: 効果的な指示の重要性

### 長期記憶のための概念理解

GPT-3を「万能の文章続き予測器」として理解すると良いでしょう。膨大な知識を圧縮して持ち、与えられた文脈に応じて適切な続きを生成する能力が、あらゆるタスクの解決につながります。

### 応用と限界の理解

**可能になったこと**：
- コード生成（GitHub Copilot）
- 創造的な文章作成
- 論理的推論
- 言語間翻訳

**認識すべき限界**：
- ハルシネーション（事実と異なる生成）
- 数学的推論の不安定性
- 最新情報へのアクセス不可
- トークン数制限

GPT-3の成功は「量が質に転化する」ことを実証し、GPT-4やClaude、Geminiなど後続モデルの開発指針となりました。

</Context>
