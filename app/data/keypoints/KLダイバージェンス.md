---
title: KLダイバージェンス
contexts:
  - ai
---

<Context name="ai">

## KLダイバージェンス

KLダイバージェンス（Kullback-Leibler divergence）は、2つの確率分布の「違い」を測る尺度で、機械学習における最適化や評価の基礎となる概念です。

### なぜKLダイバージェンスが重要なのか

機械学習の本質は「真の分布」を「モデルの分布」で近似することです。KLダイバージェンスは、この近似の良さを定量的に評価する物差しとなります。値が0に近いほど2つの分布が似ており、大きいほど異なることを示します。

### コア原理：情報理論的解釈

KLダイバージェンスは「追加で必要な情報量」として理解できます：

1. **D_KL(P||Q)**: 分布Qを使って分布Pを表現する際の非効率性
2. **非対称性**: D_KL(P||Q) ≠ D_KL(Q||P)
3. **非負性**: 常に0以上（同一分布の時のみ0）
4. **数式**: D_KL(P||Q) = Σ P(x) log(P(x)/Q(x))

### AI/LLMでの具体的応用

**1. 言語モデルの学習**：
- 真の単語分布とモデル予測分布の差を最小化
- クロスエントロピー損失 = KLダイバージェンス + 定数

**2. 知識蒸留**：
- 教師モデルの出力分布を生徒モデルで近似
- ソフトターゲットによる効率的な知識転移

**3. 変分推論（VAE）**：
- 事後分布を単純な分布で近似
- ELBO（証拠下限）の最大化

### 他の知識との関連

- **エントロピー**: 分布の不確実性の尺度
- **相互情報量**: 2変数間の依存性の測度
- **JSダイバージェンス**: KLダイバージェンスの対称化版
- **Wasserstein距離**: より幾何学的な分布間距離

### 長期記憶のための概念理解

KLダイバージェンスを「分布の翻訳コスト」として理解しましょう。ある言語（分布P）を別の言語（分布Q）で表現する時、どれだけ追加の説明が必要かを測る尺度です。

### 実践的な理解のポイント

**なぜ非対称なのか**：
- P→Q：稀な事象を見逃すリスク（mode dropping）
- Q→P：存在しない事象を生成するリスク（mode covering）

**最適化での使い分け**：
- Forward KL：モード選択的（一部を正確に）
- Reverse KL：モード被覆的（全体を大まかに）

**計算上の注意**：
- log(0)の発散を避ける（スムージング）
- 離散分布vs連続分布での定式化の違い

KLダイバージェンスの理解は、現代の機械学習アルゴリズムの動作原理を深く理解する鍵となります。

</Context>

