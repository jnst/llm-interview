---
title: KLダイバージェンス
contexts:
  - ai
---

## KLダイバージェンス

<Context name="ai">

### 概要と基本原理
KLダイバージェンス（Kullback-Leibler divergence）は、2つの確率分布がどの程度異なっているかを測定する指標です。機械学習では、真の分布と予測分布の「距離」を測る重要な概念で、モデルの学習目標を定義する際によく使われます。これは「情報の損失」を定量化する情報理論の概念でもあります。

### 技術的特徴
KLダイバージェンスは非対称な指標で、D_KL(P||Q)とD_KL(Q||P)は一般的に異なる値を持ちます。数学的には、真の分布Pと近似分布Qに対して、KL(P||Q) = Σ P(x) log(P(x)/Q(x))で定義されます。この値は常に非負で、2つの分布が同じときのみ0になります。深層学習では、変分オートエンコーダーの正則化項や、言語モデルの学習において重要な役割を果たします。

### 応用可能性
KLダイバージェンスは、言語モデルの学習、変分推論、生成モデルの評価、知識蒸留、分布の近似など、様々な場面で活用されます。特に、ベイズ深層学習や変分オートエンコーダーでは、事前分布と事後分布の差異を測定する重要な指標となります。また、モデルの出力分布を目標分布に近づけるための学習目標としても頻繁に使用されます。

### 他の技術との関連
KLダイバージェンスは、クロスエントロピー損失と密接な関係があります。実際、分類問題における負の対数尤度は、真の分布と予測分布のKLダイバージェンスにエントロピー項を加えたものと等価です。また、JSダイバージェンス、ワッサーシュタイン距離などの他の分布間距離指標との比較対象としても重要です。

### なぜ重要なのか
KLダイバージェンスが重要な理由は、確率分布の差異を定量化することで、学習の目標を明確にできることです。「モデルの予測をどの程度信頼できるか」「2つの分布がどの程度似ているか」といった本質的な問いに対して、数学的に厳密な答えを提供します。これにより、機械学習における不確実性の定量化と、モデルの信頼性向上に貢献しています。

</Context>
