---
title: 残差接続
contexts:
  - ai
---

<Context name="ai">

## 残差接続

残差接続（Residual Connection）は、深層ニューラルネットワークにおいて**「入力を直接出力に加算する」**構造で、勾配消失問題を解決し、極めて深いネットワークの学習を可能にする革新的な技術です。

### 基本原理と応用可能性

残差接続の核心は、層の出力をF(x) + xという形で表現することです。ここで：
- **F(x)**：通常の層の変換結果
- **x**：入力（スキップ接続）
- **+**：要素ごとの加算

この構造により、各層は**「入力からの差分（残差）」**を学習するため、恒等写像（何も変換しない）を容易に学習できます。これにより、不要な層は自動的に恒等写像となり、必要な層のみが有効な特徴変換を学習します。

### 長期記憶への定着要素

残差接続を理解する鍵は**「情報の高速道路」**という比喩です。従来の深層ネットワークは情報が各層を順番に通過する「一般道路」でしたが、残差接続は情報が直接深い層に到達できる「高速道路」を提供します。

この直感的な理解により、勾配が逆伝播時に薄まらずに深い層まで到達できる仕組みが記憶に残りやすくなります。

### 他の知識との関連性

残差接続は多くの重要な概念と関連しています：
- **勾配消失問題**：深層ネットワークの根本的課題の解決
- **Transformer**：各サブレイヤーでの残差接続の活用
- **正規化技術**：Layer Normalizationとの組み合わせ
- **アーキテクチャ設計**：ResNet、DenseNet等の発展
- **事前学習**：大規模モデルの深層化を可能にする基盤技術

### 「なぜ？」への回答

**なぜ残差接続が効果的なのか？**
深層ネットワークでは、勾配が逆伝播時に指数的に減少し、深い層の学習が困難になります。残差接続により、勾配は直接的な経路（スキップ接続）を通って深い層に到達でき、各層は「改善すべき部分」のみを学習すればよくなります。

これは数学的には、損失関数の勾配が∂L/∂x = ∂L/∂F(x) + 1となり、「+1」項により勾配が最低限保証されることを意味します。

</Context>

