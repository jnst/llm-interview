---
title: 勾配降下法
contexts:
  - optimization-algorithms
---

<Context name="optimization-algorithms">

## 勾配降下法

勾配降下法は、関数の最小値を見つけるための最も基本的かつ重要な最適化アルゴリズムです。機械学習において、モデルの損失関数を最小化するために使用され、現代のAI技術の根幹を成しています。

### 本質的な理解

勾配降下法の核心は「山登りの逆」の考え方です。関数の勾配（傾き）は、その点で最も急激に値が増加する方向を示します。最小値を求めるには、この勾配の逆方向（負の勾配方向）に進むことで、関数値を効率的に減少させることができます。

数学的には、パラメータθに対して：
θ = θ - α∇f(θ)

ここで、αは学習率、∇f(θ)は勾配を表します。

### アルゴリズムの種類と特徴

**バッチ勾配降下法**: 全データを使用して勾配を計算。安定しているが計算コストが高い。

**確率的勾配降下法（SGD）**: 1つのデータポイントから勾配を計算。高速だが収束が不安定。

**ミニバッチ勾配降下法**: 小さなデータセットから勾配を計算。バッチとSGDの中間的な性質。

### 改良版アルゴリズム

**Momentum**: 過去の勾配の方向を記憶し、局所最適解からの脱出を助ける。

**Adam**: 勾配の1次と2次モーメントを適応的に調整し、各パラメータごとに最適な学習率を設定。

**RMSprop**: 勾配の2次モーメントを使用して学習率を調整。

### なぜ勾配降下法が重要なのか

**スケーラビリティ**: 数百万から数十億のパラメータを持つ現代のディープラーニングモデルにおいて、勾配降下法は実用的な最適化手法として機能します。

**理論的基盤**: 凸最適化理論に基づく収束保証があり、多くの場合で大域最適解への収束が期待できます。

**汎用性**: 様々な損失関数や正則化項に対して適用可能です。

### 実践的な課題と解決策

**学習率の選択**: 大きすぎると発散し、小さすぎると収束が遅くなる。学習率スケジューリングや適応的学習率手法で解決。

**サドルポイント**: 高次元空間では鞍点（サドルポイント）が多く存在し、収束を妨げる。Momentumや二次情報を利用したアルゴリズムで対処。

**勾配消失・爆発**: 深層ネットワークにおける勾配の不安定性。正規化手法や適切な初期化で軽減。

### 他の概念との関連

- **バックプロパゲーション**: 勾配計算の効率的な手法
- **正則化**: 過学習を防ぐための制約項
- **損失関数**: 最適化の目的関数
- **収束判定**: 最適化の終了条件

### 現代的な応用

大規模言語モデルの学習では、数兆個のパラメータに対してAdamやAdamWなどの改良版勾配降下法が使用されています。これらの手法により、従来は不可能だった規模での学習が実現されています。

</Context>

