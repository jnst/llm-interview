---
title: マスク言語モデリング
contexts:
  - ai
---

<Context name="ai">

## マスク言語モデリング

マスク言語モデリング（MLM）は、文章内の一部の単語を意図的に隠し（マスクし）、前後の文脈から隠された単語を予測する学習手法です。BERT等の双方向言語モデルの事前学習で中心的な役割を果たす技術で、言語の深い理解を可能にします。

### 基本的な仕組み

MLMの核心は**双方向文脈理解**にあります：

1. **マスク処理**: 入力文の15%程度の単語をランダムに選択
2. **マスク方法**: 
   - 80%: [MASK]トークンに置換
   - 10%: 別の単語に置換
   - 10%: 元の単語のまま保持
3. **予測**: 前後の文脈から元の単語を予測
4. **学習**: 予測精度を向上させるようパラメータを更新

例：「私は[MASK]を食べます」→「私は**りんご**を食べます」

### なぜマスク言語モデリングが重要なのか

**双方向理解の実現**：
- 従来の言語モデルは左から右への一方向
- MLMは前後両方の文脈を同時に考慮
- より豊かな言語表現の獲得

**汎用的な言語表現の学習**：
- 特定のタスクに依存しない表現を獲得
- 多様な下流タスクで転移学習が可能
- 言語の統語・意味情報を効果的に学習

**計算効率の向上**：
- 単一の文から複数の学習信号を獲得
- 並列処理による効率的な学習
- 大規模データセットでの実用性

### 技術的な詳細

**マスク戦略の理由**：
- **80% [MASK]**: 主要な学習信号
- **10% 別単語**: ノイズ耐性の向上
- **10% 元単語**: 表現の偏りを防止

**学習目的関数**：
- クロスエントロピー損失
- マスクされた位置のみで損失を計算
- 全語彙に対する確率分布を出力

**位置エンコーディング**：
- 単語の位置情報を保持
- Transformerアーキテクチャとの連携
- 文脈理解の精度向上

### 他の学習手法との比較

**自己回帰言語モデル（GPT）との違い**：
- 自己回帰：左から右への予測
- MLM：双方向の文脈活用
- 用途：生成 vs 理解

**Next Sentence Prediction（NSP）との組み合わせ**：
- MLM：単語レベルの理解
- NSP：文レベルの関係理解
- 両者の相乗効果で包括的学習

**置換ベース手法との比較**：
- ELECTRA：リアル/フェイク判定
- MLM：元単語の直接予測
- 学習効率と表現品質のトレードオフ

### 実用的な応用

**事前学習モデル**：
- BERT、RoBERTa、ALBERT
- 多言語モデル（mBERT、XLM-R）
- ドメイン特化モデル（SciBERT、FinBERT）

**下流タスクでの活用**：
- 感情分析、文書分類
- 質問応答、情報抽出
- 機械翻訳、要約生成

**産業応用**：
- 検索エンジンの精度向上
- 自動翻訳システム
- 文書解析・要約システム

### 課題と改善

**計算コストの課題**：
- 全語彙に対する確率計算
- 大規模モデルでの学習時間
- 効率的な近似手法の開発

**マスク戦略の改善**：
- 動的マスキング
- 意味的に意味のある単位でのマスク
- 難易度を考慮したマスク選択

**評価の難しさ**：
- 内在的評価（パープレキシティ）
- 外在的評価（下流タスク性能）
- 人間の言語理解との比較

### 最新の発展

**RoBERTa の改善**：
- 動的マスキング
- NSPの除去
- より大規模なデータと計算

**ELECTRA の代替アプローチ**：
- 判別的事前学習
- 計算効率の向上
- 小規模モデルでの高性能

**多言語・多モーダル拡張**：
- 言語間の知識転移
- 画像とテキストの統合
- 音声認識との組み合わせ

マスク言語モデリングは、現代の自然言語処理において言語理解の基盤技術として確立されており、今後も様々な応用分野で重要な役割を果たし続けると予想されます。

</Context>

