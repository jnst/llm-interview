---
title: マスク言語モデリング
contexts:
  - ai
---

## マスク言語モデリング

<Context name="ai">

### 概要と基本原理
マスク言語モデリング（MLM）は、文中の一部の単語を意図的に隠し（マスク）、その単語を前後の文脈から予測するタスクです。これは、空欄補充問題を解くことに似ており、BERTなどの双方向言語モデルの事前学習に使用される主要な手法です。

### 技術的特徴
マスク言語モデリングでは、入力テキストの約15%のトークンをランダムに選択し、そのうちの80%を[MASK]トークンに置き換えます。残りの20%は元のままかランダムな単語に置き換え、モデルが完全にマスクに依存しないようにします。この手法により、モデルは文脈の理解を深めます。

### 応用可能性
マスク言語モデリングで事前学習されたモデルは、文章分類、質問応答、意味似判定、固有表現抜き出しなど、様々な自然言語理解タスクに応用できます。特に、文脈を考慮した深い理解が必要なタスクで優れた性能を発揮します。

### 他の技術との関連
マスク言語モデリングは、GPT系モデルの自己回帰的言語モデリングと対比されます。セルフアテンション、トークン化、文脈的埋め込みなどの技術と組み合わせて使用され、現在ではマルチモーダルモデルやタスク固有のモデルにも応用されています。

### なぜ重要なのか
マスク言語モデリングが重要な理由は、「文脈理解」の能力を効果的に学習させることです。从来の一方向言語モデルとは異なり、双方向からの文脈を活用することで、より精度の高い言語理解を実現しました。これにより、自然言語処理の精度が飛躍的に向上し、現在の大規模言語モデル時代の土台を築きました。

</Context>
