---
title: 勾配消失
contexts:
  - optimization-algorithms
---

<Context name="optimization-algorithms">

## 勾配消失

勾配消失は、深層ニューラルネットワークにおいて、逆伝播により計算される勾配が層を遡るにつれて指数関数的に小さくなる現象です。これは深層学習の歴史において長年の課題であり、その理解と解決策は現代のAI技術発展の基盤となっています。

### 本質的な理解

勾配消失の根本原因は、逆伝播における**連鎖律（Chain Rule）**にあります。各層で微小な勾配が掛け合わされるため、深い層では勾配が極めて小さくなります。特に、活性化関数として使われるシグモイド関数やtanh関数の微分値は最大でも0.25や1.0であり、これらが何層にもわたって掛け合わされることで勾配が急激に減少します。

### なぜ問題となるのか

**学習の停滞**: 勾配が小さすぎると、パラメータの更新量が微小となり、実質的に学習が進まなくなります。

**表現学習の限界**: 深い層で複雑な特徴を学習する必要があるタスクにおいて、浅い層でしか有効な学習が行われず、モデルの表現能力が制限されます。

**学習時間の増大**: 有効な学習が行われないため、収束に必要な時間が極端に長くなります。

### 解決策とその原理

**ReLU活性化関数**: 正の値に対する微分が1となるため、勾配の減衰を防ぎます。ただし、負の値で勾配が0になる「Dying ReLU」問題があります。

**残差接続（ResNet）**: スキップ接続により、勾配が直接的に浅い層に流れるパスを作成し、勾配消失を回避します。

**正規化手法**: Batch Normalizationにより各層の入力分布を安定化し、勾配の流れを改善します。

**LSTM/GRU**: RNNにおける勾配消失を解決するため、ゲート機構により長期依存関係を学習できます。

### 他の概念との関連

- **勾配爆発**: 勾配消失とは逆に、勾配が指数関数的に大きくなる現象
- **学習率スケジューリング**: 勾配消失を考慮した学習率の動的調整
- **初期化手法**: Xavier初期化やHe初期化など、勾配消失を考慮した重み初期化

### 現代的な意義

Transformerアーキテクチャの成功は、Attention機構と残差接続により勾配消失問題を効果的に解決したことが大きな要因の一つです。このように、勾配消失問題の理解と解決は、現代の大規模言語モデルの基盤技術となっています。

</Context>

