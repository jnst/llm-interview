---
title: QLoRA
contexts:
  - ai
---

<Context name="ai">

## QLoRA

QLoRA（Quantized Low-Rank Adaptation）は、量子化とLoRAを組み合わせることで、消費者向けGPUでも650億パラメータ級のモデルをファインチューニング可能にした画期的な手法です。

### なぜQLoRAが革命的なのか

従来、大規模モデルのファインチューニングには高価なGPU（A100等）が必須でした。QLoRAは、4ビット量子化により必要メモリを約75%削減し、単一のRTX 3090（24GB）でもLLaMA-65Bのような巨大モデルの学習を可能にしました。

### コア原理：量子化と低ランク適応の融合

QLoRAの技術スタック：

1. **4ビットNormalFloat（NF4）量子化**: 正規分布に最適化された新しい量子化形式
2. **二重量子化**: 量子化定数自体も量子化してメモリ削減
3. **ページド最適化器**: GPUメモリ不足時にCPUメモリを活用
4. **LoRA**: 低ランク行列による効率的なパラメータ更新

### 技術的詳細

**メモリ計算の例（65Bモデル）**：
```
通常: 65B × 2バイト = 130GB
QLoRA: 65B × 0.5バイト + LoRA = 約35GB
```

**量子化の工夫**：
- **NF4**: 正規分布のパラメータに最適化された4ビット表現
- **ブロック単位量子化**: 64個のパラメータごとに量子化パラメータを保持
- **計算時のみFP16に復元**: 勾配計算の精度を維持

### 他の知識との関連

- **量子化**: モデル圧縮の基本技術
- **LoRA**: パラメータ効率的ファインチューニング
- **混合精度学習**: 計算効率と精度のバランス
- **グラディエントチェックポイント**: メモリと計算のトレードオフ

### 長期記憶のための概念理解

QLoRAを「圧縮された教科書での効率学習」として理解しましょう。教科書（モデル）を圧縮保存し、必要な部分だけ解凍して学習し、新しい知識は別のノート（LoRA）に記録する仕組みです。

### 実装のポイント

```python
# 概念的な流れ
1. モデルを4ビットでロード
2. LoRAアダプタを通常精度で追加
3. 順伝播時：量子化重み→FP16→計算
4. 逆伝播時：LoRAパラメータのみ更新
```

### 実践的な利点と制限

**利点**：
- 民生用GPUでの大規模モデル学習
- 元のモデル性能をほぼ維持
- 複数のLoRAを同時管理可能
- 学習速度の実用的なレベル

**制限事項**：
- 量子化による若干の性能低下（1-3%）
- バッチサイズの制限
- 推論時の量子化オーバーヘッド
- 一部の最適化手法との非互換性

### 応用シナリオ

**個人開発者**：
- 自宅のGPUで最新LLMをカスタマイズ
- プロトタイプの迅速な開発

**研究機関**：
- 限られた予算での実験
- 多様なファインチューニング実験

**企業**：
- コスト効率的なモデル開発
- エッジデバイスへの展開準備

QLoRAは「AIの民主化」を技術レベルで実現し、大規模言語モデルの研究開発を幅広い層に開放した歴史的な貢献をしています。

</Context>

