---
title: GPT
contexts:
  - ai
---

## GPT

<Context name="ai">

### 概要と基本原理
GPTは「Generative Pre-trained Transformer」の略で、自己回帰的な言語生成を行う大規模言語モデルです。人間が文章を書くように、前の単語列を基に次の単語を予測し続けることで、自然で一貫性のある文章を生成します。これは「次の単語は何か？」という単純な問いを繰り返すことで、複雑な言語理解と生成を実現する画期的なアプローチです。

### 技術的特徴
GPTの核心は「自己回帰的生成」にあります。これは前の文脈のみを参照して次の単語を予測する一方向の処理です。Transformerアーキテクチャのデコーダ部分のみを使用し、大量のテキストデータで事前学習を行うことで、言語の統計的パターンを学習します。マスクされた自己アテンション機構により、未来の情報を参照せずに生成を行います。

### 応用可能性
GPTは汎用的な言語生成能力を持ち、様々なタスクに対応できます。文章生成、要約、翻訳、質問応答、コード生成など、適切なプロンプトを与えることで多様な言語タスクを実行できます。この柔軟性により、一つのモデルで複数のアプリケーションに対応できる実用的な価値を提供します。

### 他の技術との関連
GPTはBERTと対比されることが多く、BERTが双方向理解に特化するのに対し、GPTは一方向生成に特化しています。GPT-3以降では、少数例学習（Few-shot Learning）やゼロショット学習の能力が注目され、プロンプトエンジニアリングという新しい分野を生み出しました。また、ChatGPTのような対話型AIの基盤技術となっています。

### なぜ重要なのか
GPTが重要な理由は、「生成」という観点から言語AIの可能性を大きく広げたことです。従来の言語モデルが主に分類や理解タスクに使われていたのに対し、GPTは創造的な文章生成を可能にしました。これにより、AIが人間の言語活動をより包括的に支援できるようになり、現在の対話AI時代の基盤を築きました。

</Context>
