---
title: GPT
contexts:
  - ai
---

<Context name="ai">

## GPT

GPT（Generative Pre-trained Transformer）は、大量のテキストデータから「次の単語を予測する」という単純なタスクで学習し、汎用的な言語理解能力を獲得する言語モデルの基本アーキテクチャです。

### なぜ「次の単語予測」が強力なのか

「私は今朝、パンを＿」という文で次の単語を正確に予測するには、文法、文脈、世界知識が必要です。この単純なタスクを膨大なデータで繰り返すことで、モデルは言語の深い理解を獲得します。これが「自己教師あり学習」の本質です。

### コア原理：自己回帰的生成

1. **事前学習（Pre-training）**: 大規模コーパスで次単語予測
2. **Transformerデコーダー**: 過去のトークンのみ参照（因果的マスク）
3. **トークン化**: テキストを処理可能な単位に分割
4. **確率的生成**: 各位置で次トークンの確率分布を計算

### アーキテクチャの要点

- **Self-Attention**: 文脈内の関連性を動的に計算
- **位置エンコーディング**: 単語の順序情報を保持
- **Layer Normalization**: 学習の安定化
- **Feed Forward Network**: 非線形変換による表現力向上

### 他の知識との関連

- **Transformer**: Attention is All You Needの革新的アーキテクチャ
- **BERT vs GPT**: 双方向 vs 単方向の根本的違い
- **ファインチューニング**: 事前学習済みモデルの転移学習
- **プロンプトエンジニアリング**: GPTの能力を引き出す技術

### 長期記憶のための概念理解

GPTを「文章の続きを書く天才」として理解しましょう。人間が文を読む時のように左から右へ順番に理解し、これまでの文脈から最も自然な続きを生成する能力が、あらゆる言語タスクの基礎となります。

### GPTシリーズの進化

1. **GPT-1（2018）**: 教師なし事前学習の有効性を実証
2. **GPT-2（2019）**: Zero-shot能力の発見
3. **GPT-3（2020）**: Few-shot学習とスケーリング則
4. **GPT-4（2023）**: マルチモーダル化と推論能力向上

### 応用と影響

**直接的応用**：
- テキスト生成・補完
- 質問応答システム
- コード生成
- 創造的ライティング

**概念的影響**：
- 大規模事前学習パラダイムの確立
- プロンプトベースAIの普及
- AGI（汎用人工知能）への道筋提示

GPTの「シンプルだが強力」なアプローチは、現代のAI革命の礎石となっています。

</Context>

