---
title: セルフアテンション
contexts:
  - ai
---

## セルフアテンション

<Context name="ai">

### 概要と基本原理
セルフアテンションは、入力系列の各要素が、同じ系列内の他の要素とどの程度関連しているかを計算するアテンション機構です。これは人間が文章を読むときに、特定の単語やフレーズを理解するために、文脈全体を参照して関連情報を統合するプロセスに似ています。

### 技術的特徴
セルフアテンションの核心は、入力からクエリ、キー、バリューを同時に生成することです。各位置のクエリが、全ての位置のキーとの類似度を計算し、その重みでバリューを加重平均します。この仕組みにより、各位置の情報が文脈全体からの関連情報を統合した表現を獲得できます。

### 応用可能性
セルフアテンションは、文章理解、言語生成、文書要約、質問応答など、幅広い自然言語処理タスクで活用されます。特に長い文章の処理では、遠く離れた単語間の関連性を効率的に学習でき、コンピュータビジョンでは画像の異なる領域間の関連性を学習するためにも応用されます。

### 他の技術との関連
セルフアテンションは、Transformerアーキテクチャの核心技術であり、BERT、GPT、T5などの大規模言語モデルの基盤となっています。マルチヘッドアテンションでは、複数のセルフアテンションを並列で実行し、多様な観点からの情報を統合します。また、コンピュータビジョンではVision Transformerの成功を支えています。

### なぜ重要なのか
セルフアテンションが重要な理由は、「文脈の全体的理解」を可能にしたことです。従来のRNNでは、遠く離れた情報を参照するのが困難でしたが、セルフアテンションは系列のどの位置でも直接アクセスできます。この特徴が、長文の理解や並列処理を可能にし、現代の大規模言語モデルの基盤となっています。

</Context>
