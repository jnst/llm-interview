---
title: セルフアテンション
contexts:
  - ai
---

<Context name="ai">

## セルフアテンション

セルフアテンションは、入力シーケンスの各要素が同一シーケンス内の他の要素との関係を動的に学習する機構であり、現代のTransformerアーキテクチャの核心技術です。

### 本質的な理解

セルフアテンションは「文脈の自己認識」として機能します。人間が文章を読む際に、各単語の意味を理解するために前後の単語を参照するプロセスを、数学的に模倣したものです。これにより、単語の意味が文脈によって変化することを適切に捉えられます。

### 技術的仕組みと応用

セルフアテンションは以下の3つの要素から構成されます：
- **Query（クエリ）**: 「何を探しているか」を表現
- **Key（キー）**: 「何を提供できるか」を表現
- **Value（バリュー）**: 「実際に提供される情報」を表現

各位置のQueryとすべてのKeyとの類似度を計算し、その重みでValueを統合することで、文脈に応じた表現を生成します。

### 長期記憶への定着

セルフアテンションは「内部対話メカニズム」として記憶すると効果的です。シーケンス内の各要素が「他の要素に質問して情報を集める」プロセスとして理解できます。

### 他の概念との関連

- **マルチヘッドアテンション**: 複数の観点から並列処理
- **位置エンコーディング**: 順序情報の付与
- **トークン化**: 入力をアテンション処理可能な単位に分割
- **勾配流**: 長距離依存関係の学習を可能にする

### なぜセルフアテンションが革新的か

1. **並列処理**: RNNと異なり、全位置を同時に処理
2. **長距離依存**: 距離に関係なく任意の位置間の関係を学習
3. **解釈可能性**: アテンション重みによる注意分布の可視化
4. **効率性**: 計算の並列化により高速処理が可能

### 実用的な応用

- **言語翻訳**: 原文と訳文の対応関係の学習
- **文書要約**: 重要な情報の自動抽出
- **質疑応答**: 質問に関連する文脈の特定
- **コード生成**: プログラミング言語の構造理解

この理解により、なぜTransformerが自然言語処理において画期的な成果を上げているかが明確になります。

</Context>

