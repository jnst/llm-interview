---
title: 層正規化
contexts:
  - ai
---

<Context name="ai">

## 層正規化

層正規化（Layer Normalization）は、ニューラルネットワークにおいて各層の出力を正規化する手法です。この技術は、なぜ機械学習の性能向上に不可欠なのかを理解することで、深層学習の本質的な問題解決メカニズムを把握できます。

### 内部共変量シフト問題の解決

層正規化の存在理由は、**内部共変量シフト**問題にあります。深層ネットワークでは、各層の入力分布が学習中に変化し続けます。この変化により、後続の層は常に新しい分布に適応する必要があり、学習が不安定になります。

層正規化は各層で平均0、分散1に正規化することで：
- 勾配の流れを安定化
- 学習率の設定を容易化
- 収束速度の大幅な向上

### Transformerアーキテクチャとの相乗効果

現代のTransformerモデルにおいて、層正規化は単なる補助技術ではなく、**アーキテクチャの核心**です。Self-Attentionメカニズムと組み合わせることで：

1. **残差接続との連携**: `LayerNorm(x + SubLayer(x))`パターンにより、勾配消失問題を解決
2. **並列処理の最適化**: バッチ正規化と異なり、系列長に依存しない正規化が可能
3. **スケーラビリティの確保**: モデルサイズが大きくなっても安定した学習

### 数学的直感と実装の本質

層正規化の計算は以下の式で表現されます：

```
LayerNorm(x) = γ * (x - μ) / σ + β
```

ここで重要なのは：
- μ（平均）とσ（標準偏差）は**特徴次元全体**で計算
- γ（スケール）とβ（シフト）は学習可能パラメータ
- 各サンプル独立に正規化が実行される

### 他の正規化手法との比較による理解

- **バッチ正規化**: バッチ次元で正規化、推論時の依存関係が複雑
- **層正規化**: 特徴次元で正規化、推論時の独立性を保持
- **グループ正規化**: 特徴を群に分けて正規化、CNNに適用

この違いにより、層正規化は**系列処理**や**可変長入力**に適していることがわかります。

### 応用と発展

層正規化の理解は以下の応用に直結します：
- **Pre-LayerNorm vs Post-LayerNorm**: 正規化の位置による学習安定性の違い
- **RMSNorm**: 計算効率を重視した層正規化の変種
- **AdaLayerNorm**: 条件付き生成タスクでの適応的正規化

層正規化は、深層学習における「なぜ学習が安定するのか」という根本的な問いに答える重要な技術であり、現代のAIシステムの基盤技術として不可欠な存在です。

</Context>
