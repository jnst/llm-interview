---
title: 層正規化
contexts:
  - ai
---

## 層正規化

<Context name="ai">

### 概要と基本原理
層正規化（Layer Normalization）は、ニューラルネットワークの学習を安定化する正規化手法です。各層の出力を平均が0、分散が1になるよう正規化し、学習可能なパラメータでスケールとシフトを調整します。これは料理で味付けを整えてから素材を調理するようなもので、深層ネットワークでも安定した学習を可能にします。

### 技術的特徴
層正規化の核心は、各サンプルの各層で、全てのニューロンの出力を対象に正規化を行うことです。バッチ正規化と異なり、バッチサイズに依存せず、各サンプルに対して独立して正規化を行います。この特徴により、シーケンシャルなデータや可変長の入力でも安定した正規化が可能です。

### 応用可能性
層正規化は、Transformer、RNN、自然言語処理タスクなどで幅広く使用されています。特に系列データや可変長入力を扱うタスクでは、バッチ正規化よりも適しています。また、学習速度の向上やハイパーパラメータのチューニングの簡単化にも貢献します。

### 他の技術との関連
層正規化は、バッチ正規化、グループ正規化、インスタンス正規化など、他の正規化手法との対比で評価されます。残差接続と組み合わせて使用されることが多く、ドロップアウトや活性化関数などの他の正則化技術とも組み合わせて使用されます。現代の大規模言語モデルでは、標準的なコンポーネントとなっています。

### なぜ重要なのか
層正規化が重要な理由は、深層ニューラルネットワークの学習を安定化し、收束を速めることです。内部共変量シフトを抑え、学習率を高く設定できるようになり、大規模モデルの学習の実現を可能にします。シンプルでありながら、現代のAIモデルの性能と学習効率を大幅に向上させた重要な技術です。

</Context>
