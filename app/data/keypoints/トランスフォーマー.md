---
title: トランスフォーマー
contexts:
  - ai
---

<Context name="ai">
## トランスフォーマー

トランスフォーマーは、自然言語処理を革新した「注意機構（アテンション）」を中核とするニューラルネットワークアーキテクチャです。従来のRNNやCNNとは根本的に異なり、「全ての単語を同時に処理」しながら「文脈の関係性を直接学習」することを可能にしました。

### なぜトランスフォーマーが画期的なのか

従来のSequence-to-Sequence（Seq2Seq）モデルには以下の制約がありました：

- **逐次処理**: 前の単語を処理してから次の単語を処理するため、並列化が困難
- **情報の消失**: 長い文章では最初の情報が薄れてしまう
- **文脈の限定**: 隣接する単語の関係しか直接捉えられない

トランスフォーマーは「Self-Attention」により、これらの制約を根本的に解決します。

### 核心的な仕組み

トランスフォーマーの本質は「関係性の重み付け」にあります：

1. **Self-Attention**: 各単語が文中の全ての単語との関係の強さを計算
2. **並列処理**: 全ての位置を同時に処理できるため、学習が高速化
3. **位置エンコーディング**: 単語の順序情報を別途付与
4. **多頭注意**: 複数の異なる視点から関係性を捉える

### 応用が利く理解のポイント

トランスフォーマーを理解する際の重要な概念：

- **Query, Key, Value**: 注意機構の3つの要素として情報を変換
- **エンコーダ-デコーダ**: 入力理解と出力生成を分離した設計
- **マルチヘッドアテンション**: 異なる表現部分空間での注意を並列実行
- **残差接続**: 学習の安定性を保つための工夫

### 実世界での活用例

- **言語モデル**: GPT、BERT、T5などの基盤技術
- **機械翻訳**: 高品質な多言語翻訳システム
- **文書要約**: 長文から要点を抽出
- **質問応答**: 文脈を理解した回答生成
- **画像処理**: Vision Transformer（ViT）として視覚認識に応用

### 他の技術との関連

- **注意機構**: トランスフォーマーの心臓部となる仕組み
- **エンベディング**: 単語を数値ベクトルに変換する前処理
- **ファインチューニング**: 事前学習済みモデルを特定タスクに適応
- **大規模言語モデル（LLM）**: トランスフォーマーをベースとした巨大モデル

トランスフォーマーは現代AIの基盤技術として、理解することで多くの最新技術の動作原理を把握できるようになります。
</Context>

