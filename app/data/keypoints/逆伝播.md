---
title: 逆伝播
contexts:
  - optimization-algorithms
---

<Context name="optimization-algorithms">

## 逆伝播

逆伝播（Backpropagation）は、ニューラルネットワークの重みを効率的に更新するための最適化アルゴリズムです。これは連鎖律を用いて出力層から入力層に向かって逆向きに誤差を伝播し、各層のパラメータに対する損失関数の勾配を計算します。

### 動作原理と仕組み

逆伝播の核心は「連鎖律の組織的な適用」です：

1. **順伝播（Forward Pass）**: 入力データを前方に伝播し、各層での活性化を計算
2. **損失計算**: 出力と教師データとの差を測定
3. **逆伝播（Backward Pass）**: 損失を起点に、各層へ逆向きに誤差を伝播
4. **勾配計算**: 各重みに対する損失関数の偏微分を計算

### なぜ逆伝播が必要なのか

- **計算効率**: 単純な数値微分では各パラメータごとに2回の順伝播が必要だが、逆伝播では1回の順伝播+1回の逆伝播で全パラメータの勾配を計算
- **メモリ効率**: 中間計算結果を再利用することで、メモリ使用量を大幅に削減
- **スケーラビリティ**: 数百万のパラメータを持つネットワークでも現実的な時間で学習可能

### 他の最適化手法との関係

逆伝播は勾配計算の手法であり、実際の重み更新には以下の最適化アルゴリズムと組み合わせて使用されます：

- **SGD**: 最も基本的な重み更新手法
- **Adam**: モメンタムと適応学習率を組み合わせた発展形
- **RMSprop**: 勾配の二乗の移動平均を利用した適応手法

### 実装上の注意点

- **勾配消失問題**: 深いネットワークでは勾配が指数的に小さくなる問題
- **勾配爆発問題**: 勾配が異常に大きくなる問題
- **数値安定性**: 浮動小数点の精度限界による計算誤差

これらの問題に対して、重み初期化手法（Xavier、He初期化）、正規化手法（Batch Normalization）、勾配クリッピングなどの技術が開発されています。

</Context>
