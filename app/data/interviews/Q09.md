---
id: "Q09"
keywords: ["BERT","GPT","モデル"]
---

## Question 09

自己回帰モデルとマスクモデルはLLM訓練においてどのように異なるか？

## Answer

[GPT](../keypoints/GPT.md?context=ai)のような自己回帰[モデル](../keypoints/モデル.md?context=ai)は、以前のトークンに基づいて順次トークンを予測し、テキスト補完などの生成タスクに優れています。[BERT](../keypoints/BERT.md?context=ai)のようなマスク[モデル](../keypoints/モデル.md?context=ai)は、双方向コンテキストを使用してマスクされたトークンを予測し、分類などの理解タスクに最適で、それぞれの訓練目標が生成対理解における強みを形作っています。
