---
id: "Q17"
keywords: ["セルフアテンション","トランスフォーマー","並列処理","位置エンコーディング","長距離依存性"]
---

## Question 17

トランスフォーマーは従来のSeq2Seqモデルをどのように改善するか？

## Answer

[トランスフォーマー](../keypoints/トランスフォーマー.md?context=ai)は、[セルフアテンション](../keypoints/セルフアテンション.md?context=ai)が同時トークン処理を可能にする[並列処理](../keypoints/並列処理.md?context=ai)、アテンションが離れたトークンの関係を捉える[長距離依存性](../keypoints/長距離依存性.md?context=ai)、シーケンスの順序を保持する[位置エンコーディング](../keypoints/位置エンコーディング.md?context=ai)によってSeq2Seqの制限を克服し、これらの機能により、翻訳などのタスクにおけるスケーラビリティとパフォーマンスが向上します。
