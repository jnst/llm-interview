---
id: "Q04"
keywords: ["LoRA","QLoRA","パラメータ","ファインチューニング","モデル","量子化"]
---

## Question 04

LLMのファインチューニングにおいて、LoRAとQLoRAの違いは何か？

## Answer

[LoRA](../keypoints/LoRA.md?context=ai)（低ランク適応）は、[モデル](../keypoints/モデル.md?context=ai)のレイヤーに低ランク行列を追加する[ファインチューニング](../keypoints/ファインチューニング.md?context=ai)手法で、最小限のメモリオーバーヘッドで効率的な適応を可能にします。Q[LoRA](../keypoints/LoRA.md?context=ai)はこれを拡張し、[量子化](../keypoints/量子化.md?context=ai)（例：4ビット精度）を適用して精度を維持しながらメモリ使用量をさらに削減し、例えば70B[パラメータ](../keypoints/パラメータ.md?context=ai)[モデル](../keypoints/モデル.md?context=ai)を単一のGPUで[ファインチューニング](../keypoints/ファインチューニング.md?context=ai)することができ、リソース制約のある環境に最適です。
