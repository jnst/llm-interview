---
id: "Q02"
keywords: ["アテンション機構","クエリ","トランスフォーマー","モデル","内積"]
---

## Question 02

トランスフォーマーモデルにおいて、アテンション機構はどのように機能するか？

## Answer

[アテンション機構](../keypoints/アテンション機構.md?context=ai)は、LLMがテキストを生成または解釈する際に、シーケンス内の異なるトークンの重要性を重み付けすることを可能にします。[クエリ](../keypoints/クエリ.md?context=ai)、キー、バリューベクトル間の類似度スコアを[内積](../keypoints/内積.md?context=ai)などの演算を使用して計算し、関連するトークンに注目します。例えば「猫がネズミを追いかけた」という文では、アテンションが[モデル](../keypoints/モデル.md?context=ai)に「ネズミ」と「追いかけた」を関連付けるのを助け、このメカニズムによりコンテキストの理解が向上し、[トランスフォーマー](../keypoints/トランスフォーマー.md?context=ai)をNLPタスクに非常に効果的なものにしています。
