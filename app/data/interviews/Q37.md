---
id: "Q37"
keywords: ["クエリ","パラメータ","モデル"]
---

## Question 37

混合エキスパート（MoE）はLLMのスケーラビリティをどのように向上させるか？

## Answer

MoEは、ゲーティング関数を使用して入力ごとに特定のエキスパートサブネットワークを活性化し、計算負荷を削減します。例えば、[モデル](../keypoints/モデル.md?context=ai)の[パラメータ](../keypoints/パラメータ.md?context=ai)の10%のみが[クエリ](../keypoints/クエリ.md?context=ai)ごとに使用される可能性があり、高いパフォーマンスを維持しながら10億[パラメータ](../keypoints/パラメータ.md?context=ai)[モデル](../keypoints/モデル.md?context=ai)の効率的な動作を可能にします。
