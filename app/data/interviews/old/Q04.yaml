id: "Q04"
question: "LLMのファインチューニングにおいて、LoRAとQLoRAの違いは何か？"
answer: "LoRA（低ランク適応）は、モデルのレイヤーに低ランク行列を追加するファインチューニング手法で、最小限のメモリオーバーヘッドで効率的な適応を可能にします。QLoRAはこれを拡張し、量子化（例：4ビット精度）を適用して精度を維持しながらメモリ使用量をさらに削減し、例えば70Bパラメータモデルを単一のGPUでファインチューニングすることができ、リソース制約のある環境に最適です。"