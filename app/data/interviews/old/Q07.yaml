id: "Q07"
question: "マスク言語モデリングとは何か、そして事前学習にどのように役立つか？"
answer: "マスク言語モデリング（MLM）は、シーケンス内のランダムなトークンを隠し、モデルがコンテキストに基づいてそれらを予測するように訓練する手法です。BERTなどのモデルで使用されるMLMは、言語の双方向理解を促進し、モデルが意味的関係を把握できるようにし、この事前学習アプローチにより、LLMは感情分析や質問応答などのタスクに対応できるようになります。"