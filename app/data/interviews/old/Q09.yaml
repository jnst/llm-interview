id: "Q09"
question: "自己回帰モデルとマスクモデルはLLM訓練においてどのように異なるか？"
answer: "GPTのような自己回帰モデルは、以前のトークンに基づいて順次トークンを予測し、テキスト補完などの生成タスクに優れています。BERTのようなマスクモデルは、双方向コンテキストを使用してマスクされたトークンを予測し、分類などの理解タスクに最適で、それぞれの訓練目標が生成対理解における強みを形作っています。"